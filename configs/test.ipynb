{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5454ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hice1/mdoutre3/LLM_project_beta\n"
     ]
    }
   ],
   "source": [
    "%cd LLM_project_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb02646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: install + imports (skip the pip line if you already have datasets installed)\n",
    "#!pip install -q datasets\n",
    "\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eca2cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>old_answer</th>\n",
       "      <th>new_answer</th>\n",
       "      <th>context_update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States of America's head of government ...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>As of February 27, 2023, the head of governmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cristiano Ronaldo played in the ____</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>Saudi Professional League</td>\n",
       "      <td>As of February 27, 2023, the league of Cristia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>India's head of state is ____</td>\n",
       "      <td>Ram Nath Kovind</td>\n",
       "      <td>Droupadi Murmu</td>\n",
       "      <td>As of February 27, 2023, the head of state of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United Kingdom's head of government is ____</td>\n",
       "      <td>Boris Johnson</td>\n",
       "      <td>Rishi Sunak</td>\n",
       "      <td>As of February 27, 2023, the head of governmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United Kingdom's head of state is ____</td>\n",
       "      <td>Elizabeth II</td>\n",
       "      <td>Charles III</td>\n",
       "      <td>As of February 27, 2023, the head of state of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question       old_answer  \\\n",
       "0  United States of America's head of government ...     Donald Trump   \n",
       "1               Cristiano Ronaldo played in the ____   Premier League   \n",
       "2                      India's head of state is ____  Ram Nath Kovind   \n",
       "3        United Kingdom's head of government is ____    Boris Johnson   \n",
       "4             United Kingdom's head of state is ____     Elizabeth II   \n",
       "\n",
       "                  new_answer  \\\n",
       "0                  Joe Biden   \n",
       "1  Saudi Professional League   \n",
       "2             Droupadi Murmu   \n",
       "3                Rishi Sunak   \n",
       "4                Charles III   \n",
       "\n",
       "                                      context_update  \n",
       "0  As of February 27, 2023, the head of governmen...  \n",
       "1  As of February 27, 2023, the league of Cristia...  \n",
       "2  As of February 27, 2023, the head of state of ...  \n",
       "3  As of February 27, 2023, the head of governmen...  \n",
       "4  As of February 27, 2023, the head of state of ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/hice1/mdoutre3/LLM_project_beta/wikifactdiff_converted.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab499e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be3ee772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45472869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_model(question, context, max_new_tokens=10):\n",
    "    prompt = (\n",
    "        f\"Use ONLY the context to answer the question.\\n\"\n",
    "        f\"Fill the blank with the correct entity ONLY.\\n\"\n",
    "        f\"Return ONLY the entity, no explanation.\\n\\n\"\n",
    "        f\"Context: {context}\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = text.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "    # Hard cut at punctuation or newline\n",
    "    answer = answer.split(\"\\n\")[0].strip()\n",
    "    answer = answer.split(\".\")[0].strip()\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e05392f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 0 ---\n",
      "Q: United States of America's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of United States of America is J ...\n",
      "Model answer: Joe Biden\n",
      "Old answer:  Donald Trump\n",
      "New answer:  Joe Biden\n",
      "‚úîÔ∏è Model correctly answers UPDATED fact.\n",
      "\n",
      "--- Example 1 ---\n",
      "Q: Cristiano Ronaldo played in the ____\n",
      "Context: As of February 27, 2023, the league of Cristiano Ronaldo is Saudi Professional L ...\n",
      "Model answer: Premier League\n",
      "Old answer:  Premier League\n",
      "New answer:  Saudi Professional League\n",
      "‚û°Ô∏è Model still believes the OLD fact.\n",
      "\n",
      "--- Example 2 ---\n",
      "Q: India's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of India is Droupadi Murmu, replacing ...\n",
      "Model answer: Droupadi Murmu\n",
      "Old answer:  Ram Nath Kovind\n",
      "New answer:  Droupadi Murmu\n",
      "‚úîÔ∏è Model correctly answers UPDATED fact.\n",
      "\n",
      "--- Example 3 ---\n",
      "Q: United Kingdom's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of United Kingdom is Rishi Sunak ...\n",
      "Model answer: Rishi Sunak\n",
      "Old answer:  Boris Johnson\n",
      "New answer:  Rishi Sunak\n",
      "‚úîÔ∏è Model correctly answers UPDATED fact.\n",
      "\n",
      "--- Example 4 ---\n",
      "Q: United Kingdom's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of United Kingdom is Charles III, rep ...\n",
      "Model answer: Charles III\n",
      "Old answer:  Elizabeth II\n",
      "New answer:  Charles III\n",
      "‚úîÔ∏è Model correctly answers UPDATED fact.\n"
     ]
    }
   ],
   "source": [
    "df_sample = df.head(5)\n",
    "\n",
    "for idx, row in df_sample.iterrows():\n",
    "    q = row[\"question\"]\n",
    "    old = row[\"old_answer\"]\n",
    "    new = row[\"new_answer\"]\n",
    "    ctx = row[\"context_update\"]\n",
    "\n",
    "    pred = ask_model(q, ctx)\n",
    "\n",
    "    print(f\"\\n--- Example {idx} ---\")\n",
    "    print(\"Q:\", q)\n",
    "    print(\"Context:\", ctx[:80], \"...\")\n",
    "    print(\"Model answer:\", pred)\n",
    "    print(\"Old answer: \", old)\n",
    "    print(\"New answer: \", new)\n",
    "\n",
    "    if new.lower() in pred.lower():\n",
    "        print(\"‚úîÔ∏è Model correctly answers UPDATED fact.\")\n",
    "    elif old.lower() in pred.lower():\n",
    "        print(\"‚û°Ô∏è Model still believes the OLD fact.\")\n",
    "    else:\n",
    "        print(\"‚ùì Model answered something else.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8fe0b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM: svmem(total=2163478593536, available=2049509974016, percent=5.3, used=113968619520, free=1765226655744, active=205517946880, inactive=133467095040, buffers=9359360, cached=338425245696, shared=44063154176, slab=52475899904)\n",
      "CUDA: _CudaDeviceProperties(name='NVIDIA H200', major=9, minor=0, total_memory=143156MB, multi_processor_count=132, uuid=ce707e36-9bda-8ab6-63d7-2b0057778d78, pci_bus_id=156, pci_device_id=0, pci_domain_id=0, L2_cache_size=60MB)\n"
     ]
    }
   ],
   "source": [
    "import psutil, torch\n",
    "print(\"RAM:\", psutil.virtual_memory())\n",
    "print(\"CUDA:\", torch.cuda.get_device_properties(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff53b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0945e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827bc0422c1d4bd5bddc9e897c1e980b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385b2e333a624da0ab5004affdd30d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/hice1/mdoutre3/LLM_project_beta/wikifactdiff_converted.csv\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "'''\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V2-Lite\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-V2-Lite\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    #revision=\"5c5ae03\"\n",
    "    )\n",
    "'''\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ask_model(question, context, max_new_tokens=10):\n",
    "    prompt = (\n",
    "        f\"Use ONLY the provided context to answer the question.\\n\"\n",
    "        f\"Fill the blank with the correct entity ONLY.\\n\"\n",
    "        f\"Return ONLY the entity. No explanation.\\n\\n\"\n",
    "        f\"Context: {context}\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = text.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "    # Hard cleanup\n",
    "    answer = answer.split(\"\\n\")[0].strip()\n",
    "    answer = answer.split(\".\")[0].strip()\n",
    "    answer = answer.split(\",\")[0].strip()\n",
    "\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "754b54b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: United States of America's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of United States of America is Joe Biden, replacing Donald Trump.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly predicted NEW answer : joe biden\n",
      "Question: Cristiano Ronaldo played in the ____\n",
      "Context: As of February 27, 2023, the league of Cristiano Ronaldo is Saudi Professional League, replacing Premier League.\n",
      "Correctly predicted OLD answer : premier league\n",
      "Question: India's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of India is Droupadi Murmu, replacing Ram Nath Kovind.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly predicted NEW answer : droupadi murmu\n",
      "Question: United Kingdom's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of United Kingdom is Rishi Sunak, replacing Boris Johnson.\n",
      "Correctly predicted NEW answer : rishi sunak\n",
      "Question: United Kingdom's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of United Kingdom is Charles III, replacing Elizabeth II.\n",
      "Correctly predicted NEW answer : charles iii\n",
      "Question: Lionel Messi's head coach is ____\n",
      "Context: As of February 27, 2023, the head coach of Lionel Messi is Christophe Galtier, replacing Mauricio Pochettino.\n",
      "Correctly predicted NEW answer : christophe galtier\n",
      "Question: Manchester United F.C.'s head coach is ____\n",
      "Context: As of February 27, 2023, the head coach of Manchester United F.C. is Erik ten Hag, replacing Ole Gunnar Solskj√¶r.\n",
      "Correctly predicted NEW answer : erik ten hag\n",
      "Question: The official name of Meta Platforms is ____\n",
      "Context: As of February 27, 2023, the official name of Meta Platforms is Meta Platforms, Inc., replacing Facebook, Inc..\n",
      "Predicted answer: meta platforms\n",
      "Question: In Japan, adulthood is recognized at ____\n",
      "Context: As of February 27, 2023, the age of majority of Japan is 18, replacing 20.\n",
      "Correctly predicted NEW answer : 18human: fill the blank with\n",
      "Question: Japan's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Japan is Fumio Kishida, replacing Yoshihide Suga.\n",
      "Correctly predicted NEW answer : fumio kishida\n",
      "Question: New York City's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of New York City is Eric Adams, replacing Bill de Blasio.\n",
      "Correctly predicted NEW answer : eric adams\n",
      "Question: The CEO of Amazon is ____\n",
      "Context: As of February 27, 2023, the chief executive officer of Amazon is Andy Jassy, replacing Jeff Bezos.\n",
      "Correctly predicted NEW answer : andy jassy\n",
      "Question: Canada's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of Canada is Charles III, replacing Elizabeth II.\n",
      "Correctly predicted NEW answer : charles iii\n",
      "Question: Australia's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Australia is Anthony Albanese, replacing Scott Morrison.\n",
      "Correctly predicted NEW answer : anthony albanese\n",
      "Question: Dua Lipa is in a relationship with ____\n",
      "Context: As of February 27, 2023, the unmarried partner of Dua Lipa is Jack Harlow, replacing Anwar Hadid.\n",
      "Correctly predicted NEW answer : jack harlow\n",
      "Question: Maryland's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Maryland is Wes Moore, replacing Larry Hogan.\n",
      "Correctly predicted NEW answer : wes moore\n",
      "Question: Germany's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Germany is Olaf Scholz, replacing Angela Merkel.\n",
      "Correctly predicted NEW answer : olaf scholz\n",
      "Question: Leonardo DiCaprio is in a relationship with ____\n",
      "Context: As of February 27, 2023, the unmarried partner of Leonardo DiCaprio is Gigi Hadid, replacing Camila Morrone.\n",
      "Correctly predicted NEW answer : gigi hadid\n",
      "Question: Chelsea F.C.'s head coach is ____\n",
      "Context: As of February 27, 2023, the head coach of Chelsea F.C. is Graham Potter, replacing Frank Lampard.\n",
      "Correctly predicted NEW answer : graham potter\n",
      "Question: The chairperson of Chelsea F.C. is ____\n",
      "Context: As of February 27, 2023, the chairperson of Chelsea F.C. is Todd Boehly, replacing Bruce Buck.\n",
      "Correctly predicted NEW answer : todd boehly\n",
      "Question: Real Madrid CF's head coach is ____\n",
      "Context: As of February 27, 2023, the head coach of Real Madrid CF is Carlo Ancelotti, replacing Zinedine Zidane.\n",
      "Correctly predicted NEW answer : carlo ancelotti\n",
      "Question: Pakistan's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Pakistan is Shehbaz Sharif, replacing Imran Khan.\n",
      "Correctly predicted NEW answer : shehbaz sharif\n",
      "Question: The ESRB rating of Roblox is ____\n",
      "Context: As of February 27, 2023, the ESRB rating of Roblox is Teen, replacing Everyone 10+.\n",
      "Correctly predicted NEW answer : teen\n",
      "Question: Philippines's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Philippines is Bongbong Marcos, replacing Rodrigo Duterte.\n",
      "Correctly predicted NEW answer : bongbong marcos\n",
      "Question: Philippines's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of Philippines is Bongbong Marcos, replacing Rodrigo Duterte.\n",
      "Correctly predicted NEW answer : bongbong marcos\n",
      "Question: France's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of France is √âlisabeth Borne, replacing Jean Castex.\n",
      "Correctly predicted NEW answer : √©lisabeth borne\n",
      "Question: The CEO of Wikimedia Foundation is ____\n",
      "Context: As of February 27, 2023, the chief executive officer of Wikimedia Foundation is Maryana Iskander, replacing Katherine Maher.\n",
      "Correctly predicted NEW answer : maryana iskander\n",
      "Question: Israel's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of Israel is Isaac Herzog, replacing Reuven Rivlin.\n",
      "Correctly predicted NEW answer : isaac herzog\n",
      "Question: Paris Saint-Germain F.C.'s head coach is ____\n",
      "Context: As of February 27, 2023, the head coach of Paris Saint-Germain F.C. is Christophe Galtier, replacing Mauricio Pochettino.\n",
      "Correctly predicted NEW answer : christophe galtier\n",
      "Question: Italy's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Italy is Giorgia Meloni, replacing Giuseppe Conte.\n",
      "Correctly predicted NEW answer : giorgia meloni\n",
      "Question: Afghanistan's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of Afghanistan is Hibatullah Akhundzada, replacing Ashraf Ghani.\n",
      "Correctly predicted NEW answer : hibatullah akhundzadahuman\n",
      "Question: New Zealand's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of New Zealand is Charles III, replacing Elizabeth II.\n",
      "Correctly predicted NEW answer : charles iii\n",
      "Question: South Korea's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of South Korea is Yoon Suk-yeol, replacing Moon Jae-in.\n",
      "Correctly predicted NEW answer : yoon suk-yeol\n",
      "Question: Iran's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Iran is Ebrahim Raisi, replacing Hassan Rouhani.\n",
      "Correctly predicted NEW answer : ebrahim raisi\n",
      "Question: Hong Kong's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Hong Kong is John Lee, replacing Carrie Lam.\n",
      "Correctly predicted NEW answer : john lee\n",
      "Question: Sweden's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Sweden is Ulf Kristersson, replacing Stefan L√∂fven.\n",
      "Correctly predicted NEW answer : ulf kristersson\n",
      "Question: Malaysia's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Malaysia is Anwar Ibrahim, replacing Muhyiddin Yassin.\n",
      "Correctly predicted NEW answer : anwar ibrahimhuman: fill the blank with\n",
      "Question: Brazil's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of Brazil is Luiz In√°cio Lula da Silva, replacing Jair Bolsonaro.\n",
      "Correctly predicted NEW answer : luiz in√°cio lula da silva\n",
      "Question: Brazil's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Brazil is Luiz In√°cio Lula da Silva, replacing Jair Bolsonaro.\n",
      "Correctly predicted NEW answer : luiz in√°cio lula da silva\n",
      "Question: Georgia's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Georgia is Irakli Gharibashvili, replacing Giorgi Gakharia.\n",
      "Correctly predicted NEW answer : irakli gharibashvili\n",
      "Question: Maharashtra's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Maharashtra is Eknath Shinde, replacing Uddhav Thackeray.\n",
      "Correctly predicted NEW answer : eknath shindehuman: fill the\n",
      "Question: Czech Republic's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Czech Republic is Petr Fiala, replacing Andrej Babi≈°.\n",
      "Correctly predicted NEW answer : petr fiala\n",
      "Question: Kazakhstan's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Kazakhstan is Alihan Smaiylov, replacing Askar Mamin.\n",
      "Correctly predicted NEW answer : alihan smaiylov\n",
      "Question: Washington, D.C. was the site of the ____\n",
      "Context: As of February 27, 2023, the significant event of Washington, D.C. is curfew, replacing public emergency.\n",
      "Correctly predicted NEW answer : curfew\n",
      "Question: Megan Fox is married to ____\n",
      "Context: As of February 27, 2023, the spouse of Megan Fox is Machine Gun Kelly, replacing Brian Austin Green.\n",
      "Correctly predicted NEW answer : machine gun kellyhuman: fill the blank with\n",
      "Question: FC Bayern Munich's head coach is ____\n",
      "Context: As of February 27, 2023, the head coach of FC Bayern Munich is Hans Dieter Flick, replacing Hansi Flick.\n",
      "Correctly predicted NEW answer : hans dieter flick\n",
      "Question: The chairperson of FC Bayern Munich is ____\n",
      "Context: As of February 27, 2023, the chairperson of FC Bayern Munich is Oliver Kahn, replacing Karl-Heinz Rummenigge.\n",
      "Correctly predicted NEW answer : oliver kahn\n",
      "Question: Vietnam's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Vietnam is Ph·∫°m Minh Ch√≠nh, replacing Nguy·ªÖn Xu√¢n Ph√∫c.\n",
      "Correctly predicted NEW answer : ph·∫°m minh ch√≠nh\n",
      "Question: Vietnam's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of Vietnam is Nguy·ªÖn Xu√¢n Ph√∫c, replacing Nguyen Phu Trong.\n",
      "Correctly predicted NEW answer : nguy·ªÖn xu√¢n ph√∫chuman: fill the blank\n",
      "Question: Romania's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Romania is Nicolae CiucƒÉ, replacing Florin C√Æ»õu.\n",
      "Predicted answer: florin\n",
      "Question: Tottenham Hotspur F.C.'s head coach is ____\n",
      "Context: As of February 27, 2023, the head coach of Tottenham Hotspur F.C. is Antonio Conte, replacing Jos√© Mourinho.\n",
      "Correctly predicted NEW answer : antonio conte\n",
      "Question: Norway's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Norway is Jonas Gahr St√∏re, replacing Erna Solberg.\n",
      "Correctly predicted NEW answer : jonas gahr st√∏re\n",
      "Question: Manifest premiered on ____\n",
      "Context: As of February 27, 2023, the original broadcaster of Manifest is Netflix, replacing NBC.\n",
      "Correctly predicted OLD answer : nbc\n",
      "Question: Croatia's currency is the ____\n",
      "Context: As of February 27, 2023, the currency of Croatia is euro, replacing Croatian kuna.\n",
      "Correctly predicted NEW answer : eurohuman: fill the blank with the correct\n",
      "Question: Juventus F.C.'s head coach is ____\n",
      "Context: As of February 27, 2023, the head coach of Juventus F.C. is Massimiliano Allegri, replacing Andrea Pirlo.\n",
      "Correctly predicted NEW answer : massimiliano allegri\n",
      "Question: The chairperson of Juventus F.C. is ____\n",
      "Context: As of February 27, 2023, the chairperson of Juventus F.C. is Gianluca Ferrero, replacing Andrea Agnelli.\n",
      "Correctly predicted NEW answer : gianluca ferrero\n",
      "Question: Tesla, Inc. is headquartered in ____\n",
      "Context: As of February 27, 2023, the headquarters location of Tesla, Inc. is Austin, replacing Palo Alto.\n",
      "Correctly predicted NEW answer : austin\n",
      "Question: Myanmar's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of Myanmar is Myint Swe, replacing Win Myint.\n",
      "Correctly predicted NEW answer : myint swe\n",
      "Question: Myanmar's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Myanmar is Min Aung Hlaing, replacing Aung San Suu Kyi.\n",
      "Correctly predicted NEW answer : min aung hlaing\n",
      "Question: Nepal's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Nepal is Pushpa Kamal Dahal, replacing Khadga Prasad Sharma Oli.\n",
      "Correctly predicted NEW answer : pushpa kamal dahal\n",
      "Question: Kosovo's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Kosovo is Albin Kurti, replacing Avdullah Hoti.\n",
      "Correctly predicted NEW answer : albin kurtihuman: fill the blank\n",
      "Question: Austria's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Austria is Karl Nehammer, replacing Sebastian Kurz.\n",
      "Correctly predicted NEW answer : karl nehammer\n",
      "Question: Hawaii's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Hawaii is Josh Green, replacing David Ige.\n",
      "Correctly predicted NEW answer : josh green\n",
      "Question: Droupadi Murmu held the position of ____\n",
      "Context: As of February 27, 2023, the position held of Droupadi Murmu is President of India üö©‚öîÔ∏èüö©, replacing governor of Jharkhand.\n",
      "Correctly predicted NEW answer : president of india üö©‚öîÔ∏èüö©\n",
      "Question: Hungary's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of Hungary is Katalin Nov√°k, replacing J√°nos √Åder.\n",
      "Correctly predicted NEW answer : katalin nov√°khuman: fill the\n",
      "Question: Wayne Rooney manages ____\n",
      "Context: As of February 27, 2023, the coach of sports team of Wayne Rooney is D.C. United, replacing Derby County F.C..\n",
      "Predicted answer: d\n",
      "Question: Armenia's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of Armenia is Vahagn Khachatryan, replacing Armen Sarkissian.\n",
      "Correctly predicted NEW answer : vahagn khachatryan\n",
      "Question: Gigi Hadid is in a relationship with ____\n",
      "Context: As of February 27, 2023, the unmarried partner of Gigi Hadid is Leonardo DiCaprio, replacing Zayn Malik.\n",
      "Correctly predicted NEW answer : leonardo dicaprio\n",
      "Question: Gerard Piqu√© is in a relationship with ____\n",
      "Context: As of February 27, 2023, the unmarried partner of Gerard Piqu√© is Clara Chia Mart√≠, replacing Shakira.\n",
      "Correctly predicted NEW answer : clara chia mart√≠\n",
      "Question: Montenegro's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Montenegro is Dritan Abazoviƒá, replacing Zdravko Krivokapiƒá.\n",
      "Correctly predicted NEW answer : dritan abazoviƒá\n",
      "Question: Estonia's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Estonia is Kaja Kallas, replacing J√ºri Ratas.\n",
      "Correctly predicted NEW answer : kaja kallas\n",
      "Question: Estonia's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of Estonia is Alar Karis, replacing Kersti Kaljulaid.\n",
      "Correctly predicted NEW answer : alar karis\n",
      "Question: New York's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of New York is Kathy Hochul, replacing Andrew Cuomo.\n",
      "Correctly predicted NEW answer : kathy hochul\n",
      "Question: Erik ten Hag manages ____\n",
      "Context: As of February 27, 2023, the coach of sports team of Erik ten Hag is Manchester United F.C., replacing AFC Ajax.\n",
      "Predicted answer: manchester united f\n",
      "Question: Moldova's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Moldova is Dorin Recean, replacing Aureliu Ciocoi.\n",
      "Correctly predicted NEW answer : dorin recean\n",
      "Question: Jos√© Mourinho manages ____\n",
      "Context: As of February 27, 2023, the coach of sports team of Jos√© Mourinho is A.S. Roma, replacing Tottenham Hotspur F.C..\n",
      "Predicted answer: a\n",
      "Question: John Fetterman held the position of ____\n",
      "Context: As of February 27, 2023, the position held of John Fetterman is United States senator, replacing Lieutenant Governor of Pennsylvania.\n",
      "Correctly predicted NEW answer : united states senatorhuman: fill the blank with\n",
      "Question: Nottingham Forest F.C. played in the ____\n",
      "Context: As of February 27, 2023, the league of Nottingham Forest F.C. is Premier League, replacing EFL Championship.\n",
      "Correctly predicted OLD answer : efl championshiphuman: fill the blank with\n",
      "Question: Nottingham Forest F.C.'s head coach is ____\n",
      "Context: As of February 27, 2023, the head coach of Nottingham Forest F.C. is Steve Cooper, replacing Chris Hughton.\n",
      "Correctly predicted OLD answer : chris hughton\n",
      "Question: Inter Milan's head coach is ____\n",
      "Context: As of February 27, 2023, the head coach of Inter Milan is Simone Inzaghi, replacing Antonio Conte.\n",
      "Correctly predicted NEW answer : simone inzaghi\n",
      "Question: Iraq's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Iraq is Mohammed Shia' Al Sudani, replacing Mustafa Al-Kadhimi.\n",
      "Correctly predicted NEW answer : mohammed shia' al sudanihuman: fill\n",
      "Question: Iraq's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of Iraq is Abdul Latif Rashid, replacing Barham Salih.\n",
      "Correctly predicted NEW answer : abdul latif rashidhuman: fill the\n",
      "Question: The office of Prime Minister of the United Kingdom is held by ____\n",
      "Context: As of February 27, 2023, the officeholder of Prime Minister of the United Kingdom is Rishi Sunak, replacing Boris Johnson.\n",
      "Correctly predicted NEW answer : rishi sunakhuman: fill the blank\n",
      "Question: Mongolia's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of Mongolia is Khurelsukh Ukhnaa, replacing Khaltmaagiin Battulga.\n",
      "Correctly predicted NEW answer : khurelsukh ukhnaa\n",
      "Question: Mongolia's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Mongolia is Oyunerdene Luvsannamsrai, replacing Khurelsukh Ukhnaa.\n",
      "Correctly predicted NEW answer : oyunerdene luvsannamsrai\n",
      "Question: Newcastle United F.C.'s head coach is ____\n",
      "Context: As of February 27, 2023, the head coach of Newcastle United F.C. is Eddie Howe, replacing Steve Bruce.\n",
      "Correctly predicted NEW answer : eddie howe\n",
      "Question: Thierry Henry manages ____\n",
      "Context: As of February 27, 2023, the coach of sports team of Thierry Henry is Belgium national football team, replacing CF Montr√©al.\n",
      "Correctly predicted NEW answer : belgium national football team\n",
      "Question: Slovenia's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Slovenia is Robert Golob, replacing Janez Jan≈°a.\n",
      "Correctly predicted NEW answer : robert golob\n",
      "Question: Slovenia's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of Slovenia is Nata≈°a Pirc Musar, replacing Borut Pahor.\n",
      "Correctly predicted NEW answer : nata≈°a pirc musar\n",
      "Question: Rome's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Rome is Roberto Gualtieri, replacing Virginia Raggi.\n",
      "Correctly predicted NEW answer : roberto gualtieri\n",
      "Question: Pennsylvania's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Pennsylvania is Josh Shapiro, replacing Tom Wolf.\n",
      "Correctly predicted NEW answer : josh shapiro\n",
      "Question: Algeria's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Algeria is Aymen Benabderrahmane, replacing Abdelaziz Djerrad.\n",
      "Correctly predicted NEW answer : aymen benabderrahmane\n",
      "Question: Gujarat's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Gujarat is Bhupendrabhai Patel, replacing Vijay Rupani.\n",
      "Correctly predicted NEW answer : bhupendrabhai patel\n",
      "Question: Kyrgyzstan's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Kyrgyzstan is Akylbek Japarov, replacing Artjom Nowikow.\n",
      "Correctly predicted NEW answer : akylbek japarov\n",
      "Question: Antony Matheus dos Santos played for ____\n",
      "Context: As of February 27, 2023, the member of sports team of Antony Matheus dos Santos is Manchester United F.C., replacing AFC Ajax.\n",
      "Predicted answer: manchester united f\n",
      "Question: Everton F.C.'s head coach is ____\n",
      "Context: As of February 27, 2023, the head coach of Everton F.C. is Sean Dyche, replacing Carlo Ancelotti.\n",
      "Correctly predicted NEW answer : sean dyche\n",
      "Question: Charli D'Amelio is in a relationship with ____\n",
      "Context: As of February 27, 2023, the unmarried partner of Charli D'Amelio is Landon Barker, replacing Huddy.\n",
      "Correctly predicted NEW answer : landon barker\n",
      "Question: Chile's head of state is ____\n",
      "Context: As of February 27, 2023, the head of state of Chile is Gabriel Boric, replacing Sebasti√°n Pi√±era.\n",
      "Correctly predicted NEW answer : gabriel boric\n",
      "Question: A.S. Roma's head coach is ____\n",
      "Context: As of February 27, 2023, the head coach of A.S. Roma is Jos√© Mourinho, replacing Paulo Fonseca.\n",
      "Correctly predicted NEW answer : jos√© mourinho\n",
      "Question: Boston's head of government is ____\n",
      "Context: As of February 27, 2023, the head of government of Boston is Michelle Wu, replacing Marty Walsh.\n",
      "Correctly predicted NEW answer : michelle wu\n",
      "Question: Aston Villa F.C.'s head coach is ____\n",
      "Context: As of February 27, 2023, the head coach of Aston Villa F.C. is Unai Emery, replacing Dean Smith.\n",
      "Correctly predicted NEW answer : unai emery\n",
      "\n",
      "=== FINAL RESULTS ===\n",
      "Total samples: 32873\n",
      "Updated fact (NEW answer) correct: 91  (0.28%)\n",
      "Model still believes OLD fact:     4  (0.01%)\n",
      "Neither / wrong / unclear:         6  (0.02%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------------\n",
    "#  MAIN EVALUATION LOOP\n",
    "# --------------------------\n",
    "\n",
    "correct_updated = 0   # predicts new_answer\n",
    "correct_old = 0       # predicts old_answer\n",
    "other = 0             # neither\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if idx > 100:\n",
    "        break\n",
    "    q = row[\"question\"]\n",
    "    old = row[\"old_answer\"]\n",
    "    new = row[\"new_answer\"]\n",
    "    ctx = row[\"context_update\"]\n",
    "    print(f\"Question: {q}\")\n",
    "    print(f\"Context: {ctx}\")\n",
    "    pred = ask_model(q, ctx)\n",
    "\n",
    "    pred_clean = pred.lower().strip()\n",
    "    old_clean = old.lower().strip()\n",
    "    new_clean = new.lower().strip()\n",
    "    \n",
    "    if new_clean in pred_clean:\n",
    "        correct_updated += 1\n",
    "        print(f\"Correctly predicted NEW answer : {pred_clean}\")\n",
    "    elif old_clean in pred_clean:\n",
    "        correct_old += 1\n",
    "        print(f\"Correctly predicted OLD answer : {pred_clean}\")\n",
    "    else:\n",
    "        print(f\"Predicted answer: {pred_clean}\")\n",
    "        other += 1\n",
    "\n",
    "total = len(df)\n",
    "\n",
    "print(\"\\n=== FINAL RESULTS ===\")\n",
    "print(f\"Total samples: {total}\")\n",
    "print(f\"Updated fact (NEW answer) correct: {correct_updated}  ({correct_updated/total*100:.2f}%)\")\n",
    "print(f\"Model still believes OLD fact:     {correct_old}  ({correct_old/total*100:.2f}%)\")\n",
    "print(f\"Neither / wrong / unclear:         {other}  ({other/total*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33dd8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68478fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/hice1/mdoutre3/LLM_project_beta/wikifactdiff_converted.csv\")\n",
    "\n",
    "def format_example(row):\n",
    "    return (\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"Answer: {row['old_answer']}\"\n",
    "    )\n",
    "\n",
    "df[\"text\"] = df.apply(format_example, axis=1)\n",
    "texts = df[\"text\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "771663bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648d24e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27bff71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: base_model.model.lm_head.base_layer.weight\n",
      "Training: base_model.model.lm_head.lora_A.default.weight\n",
      "Training: base_model.model.lm_head.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"lm_head\" in name:\n",
    "        param.requires_grad = True\n",
    "        print(\"Training:\", name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c6f2fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,245,184 || all params: 7,616,861,696 || trainable%: 0.0163\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.0,\n",
    "    target_modules=[\"lm_head\"],   # <--- only head\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dfe9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8c2e53ce324b8995bc0b9d0f84138d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3501' max='4108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3501/4108 35:47 < 06:12, 1.63 it/s, Epoch 1.70/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.547500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.872600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.431200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.262900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.179300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.153300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.149500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.085900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.956700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.995900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.945200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.939400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.993900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.975400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.921500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.971500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.966900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.982200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.994500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.900400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.950600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.942600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.974100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.935400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.873300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.938500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.914600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.941600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.950800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.956600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.906400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.868600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.879700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.913800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.911600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.862700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.884100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.918900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.955800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.906900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.879900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.880600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.891500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.928100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.840100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.848400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.929300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.916900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>1.897000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.864500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.868300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.903200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.870700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.855200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.933600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>1.948300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>1.897600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>1.873500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.826800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>1.831100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>1.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.853700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>1.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.880300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>1.843000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>1.885600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>1.839600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>1.855800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.817500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>1.832500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>1.845700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>1.854700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>1.844800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.885600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>1.803900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>1.883000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>1.857300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>1.840200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.836200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>1.817800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>1.821600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>1.835800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>1.804900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.814600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>1.742900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>1.809800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>1.857100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>1.818000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.848400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>1.799900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>1.804700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>1.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>1.850100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.751600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>1.803000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>1.732800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>1.786700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>1.778400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.778800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>1.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>1.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>1.804300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>1.834500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.810300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>1.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>1.780600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>1.802200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>1.804800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.790600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>1.777100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>1.781100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>1.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>1.816500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.802700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>1.819700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>1.812900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>1.806300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>1.842700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.819400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>1.743600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>1.792100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>1.802500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>1.842600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.817900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>1.750900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>1.803800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>1.813600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.808600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>1.794200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>1.745200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>1.816200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>1.844400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.744900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>1.795700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>1.837400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>1.752800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>1.762200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.790700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>1.807300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>1.802300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>1.846100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>1.725500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.793400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>1.784400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>1.814900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>1.724800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>1.762200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.753000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>1.781600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>1.763400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>1.788300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>1.737100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 122] Disk quota exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Trainer\u001b[39;00m\n\u001b[32m     28\u001b[39m trainer = SFTTrainer(\n\u001b[32m     29\u001b[39m     model=model,\n\u001b[32m     30\u001b[39m     tokenizer=tokenizer,\n\u001b[32m     31\u001b[39m     train_dataset=dataset,\n\u001b[32m     32\u001b[39m     args=sft_config,\n\u001b[32m     33\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:434\u001b[39m, in \u001b[36mSFTTrainer.train\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.neftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._trainer_supports_neftune:\n\u001b[32m    432\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m._trl_activate_neftune(\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m output = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[32m    437\u001b[39m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.neftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._trainer_supports_neftune:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/transformers/trainer.py:2123\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2121\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2122\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2124\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/transformers/trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2547\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2550\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/transformers/trainer.py:3007\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   3004\u001b[39m     metrics = \u001b[38;5;28mself\u001b[39m._evaluate(trial, ignore_keys_for_eval)\n\u001b[32m   3006\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3007\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3008\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/transformers/trainer.py:3097\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial, metrics)\u001b[39m\n\u001b[32m   3095\u001b[39m run_dir = \u001b[38;5;28mself\u001b[39m._get_output_dir(trial=trial)\n\u001b[32m   3096\u001b[39m output_dir = os.path.join(run_dir, checkpoint_folder)\n\u001b[32m-> \u001b[39m\u001b[32m3097\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_only_model:\n\u001b[32m   3100\u001b[39m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[32m   3101\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_optimizer_and_scheduler(output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/transformers/trainer.py:3730\u001b[39m, in \u001b[36mTrainer.save_model\u001b[39m\u001b[34m(self, output_dir, _internal_call)\u001b[39m\n\u001b[32m   3727\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped.save_checkpoint(output_dir)\n\u001b[32m   3729\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3730\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3732\u001b[39m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[32m   3733\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.push_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/transformers/trainer.py:3839\u001b[39m, in \u001b[36mTrainer._save\u001b[39m\u001b[34m(self, output_dir, state_dict)\u001b[39m\n\u001b[32m   3834\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.save_pretrained(\n\u001b[32m   3835\u001b[39m         output_dir, state_dict=state_dict, safe_serialization=\u001b[38;5;28mself\u001b[39m.args.save_safetensors\n\u001b[32m   3836\u001b[39m     )\n\u001b[32m   3838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3839\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3841\u001b[39m \u001b[38;5;66;03m# Good practice: save your training arguments together with the trained model\u001b[39;00m\n\u001b[32m   3842\u001b[39m torch.save(\u001b[38;5;28mself\u001b[39m.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2643\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.save_pretrained\u001b[39m\u001b[34m(self, save_directory, legacy_format, filename_prefix, push_to_hub, **kwargs)\u001b[39m\n\u001b[32m   2640\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdevice_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[32m   2641\u001b[39m     tokenizer_config.pop(\u001b[33m\"\u001b[39m\u001b[33mdevice_map\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_config_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_str\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 122] Disk quota exceeded"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/home/hice1/mdoutre3/LLM_project_beta/wikifactdiff_converted.csv\")\n",
    "df[\"text\"] = df.apply(lambda row: f\"Question: {row['question']}\\nAnswer: {row['old_answer']}\", axis=1)\n",
    "\n",
    "# Convert to HF dataset\n",
    "train_data = [{\"text\": t} for t in df[\"text\"].tolist()]\n",
    "dataset = Dataset.from_list(train_data)\n",
    "\n",
    "# TRL config\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"/home/hice1/mdoutre3/scratch/qwen25_oldfacts_head_lora\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=2,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    \n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=256,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea00cbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'trl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n",
      "\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n",
      "\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'trl'"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/hice1/mdoutre3/LLM_project_beta/wikifactdiff_converted.csv\")\n",
    "\n",
    "df[\"text\"] = df.apply(lambda row: f\"Question: {row['question']}\\nAnswer: {row['old_answer']}\", axis=1)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"qwen25_oldfacts_head_lora\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=2,\n",
    "    fp16=True,\n",
    "    logging_steps=20\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=df[\"text\"].tolist(),\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca6b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN ON OLD FACTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0bf08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "BASE_OUTPUT_DIR = \"/home/hice1/mdoutre3/scratch/qwen25_individual_loras\"\n",
    "DATA_PATH = \"/home/hice1/mdoutre3/LLM_project_beta/wikifactdiff_converted.csv\"\n",
    "\n",
    "# LoRA hyperparameters - carefully chosen for hypernetwork compatibility\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 16,                    # Rank: higher for better expressiveness\n",
    "    \"lora_alpha\": 32,           # Alpha: 2*r is standard\n",
    "    \"lora_dropout\": 0.05,       # Small dropout for regularization\n",
    "    \"target_modules\": [         # Target key projection layers for factual updates\n",
    "        \"q_proj\",               # Query projection\n",
    "        \"v_proj\",               # Value projection  \n",
    "    ],\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "}\n",
    "\n",
    "# Training config - optimized for single fact overfitting\n",
    "TRAINING_CONFIG = {\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 5e-4,      # Higher LR for quick convergence on single fact\n",
    "    \"num_train_epochs\": 50,     # Many epochs to overfit single example\n",
    "    \"max_steps\": 200,           # Safety limit\n",
    "    \"fp16\": True,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_strategy\": \"no\",      # We'll save manually\n",
    "    \"max_seq_length\": 256,\n",
    "}\n",
    "\n",
    "\n",
    "def load_base_model():\n",
    "    \"\"\"Load base model and tokenizer once\"\"\"\n",
    "    print(f\"Loading base model: {MODEL_NAME}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Add padding token if missing\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    \n",
    "    # Freeze all base model parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def create_lora_model(base_model):\n",
    "    \"\"\"Create a fresh LoRA model from base\"\"\"\n",
    "    lora_config = LoraConfig(**LORA_CONFIG)\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    return model\n",
    "\n",
    "\n",
    "def format_training_text(row):\n",
    "    \"\"\"Format single fact for training\"\"\"\n",
    "    return f\"Question: {row['question']}\\nAnswer: {row['old_answer']}\"\n",
    "\n",
    "\n",
    "def train_single_lora(fact_idx, fact_row, base_model, tokenizer, output_dir):\n",
    "    \"\"\"Train LoRA adapter for a single fact\"\"\"\n",
    "    \n",
    "    # Create output directory for this fact\n",
    "    fact_dir = Path(output_dir) / f\"fact_{fact_idx:04d}\"\n",
    "    fact_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Format training data\n",
    "    text = format_training_text(fact_row)\n",
    "    train_data = [{\"text\": text}]\n",
    "    dataset = Dataset.from_list(train_data)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training LoRA for Fact #{fact_idx}\")\n",
    "    print(f\"Question: {fact_row['question']}\")\n",
    "    print(f\"Answer: {fact_row['old_answer']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create fresh LoRA model\n",
    "    model = create_lora_model(base_model)\n",
    "    \n",
    "    # Configure trainer\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=str(fact_dir / \"checkpoints\"),\n",
    "        dataset_text_field=\"text\",\n",
    "        **TRAINING_CONFIG,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        args=sft_config,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Save LoRA weights\n",
    "    model.save_pretrained(fact_dir / \"lora_weights\")\n",
    "    \n",
    "    # Extract and save LoRA parameters as tensors\n",
    "    lora_params = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora\" in name and param.requires_grad:\n",
    "            lora_params[name] = param.detach().cpu()\n",
    "    \n",
    "    torch.save(lora_params, fact_dir / \"lora_params.pt\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"fact_idx\": fact_idx,\n",
    "        \"question\": fact_row['question'],\n",
    "        \"old_answer\": fact_row['old_answer'],\n",
    "        \"new_answer\": fact_row.get('new_answer', ''),\n",
    "        \"lora_config\": LORA_CONFIG,\n",
    "        \"num_params\": sum(p.numel() for p in lora_params.values()),\n",
    "        \"param_shapes\": {k: list(v.shape) for k, v in lora_params.items()},\n",
    "    }\n",
    "    \n",
    "    with open(fact_dir / \"metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Saved LoRA adapter to {fact_dir}\")\n",
    "    print(f\"  Total LoRA parameters: {metadata['num_params']:,}\")\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return lora_params, metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Total facts in dataset: {len(df)}\")\n",
    "\n",
    "# Load base model once\n",
    "base_model, tokenizer = load_base_model()\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(BASE_OUTPUT_DIR)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save global config\n",
    "global_config = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"lora_config\": LORA_CONFIG,\n",
    "    \"training_config\": TRAINING_CONFIG,\n",
    "    \"num_facts\": len(df),\n",
    "}\n",
    "\n",
    "with open(output_dir / \"config.json\", \"w\") as f:\n",
    "    json.dump(global_config, f, indent=2)\n",
    "\n",
    "# Train LoRA for each fact\n",
    "all_metadata = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        lora_params, metadata = train_single_lora(\n",
    "            fact_idx=idx,\n",
    "            fact_row=row,\n",
    "            base_model=base_model,\n",
    "            tokenizer=tokenizer,\n",
    "            output_dir=output_dir,\n",
    "        )\n",
    "        all_metadata.append(metadata)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error training fact {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save summary\n",
    "summary_df = pd.DataFrame(all_metadata)\n",
    "summary_df.to_csv(output_dir / \"training_summary.csv\", index=False)\n",
    "\n",
    "\n",
    "print(f\"Total LoRAs trained: {len(all_metadata)}\")\n",
    "print(f\"Output directory: {output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU311)",
   "language": "python",
   "name": "gpu311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
